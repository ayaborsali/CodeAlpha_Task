# -*- coding: utf-8 -*-
"""credit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WGTiOomDtvBEBJACU_d2M79SII7KglDx



**Chargement des données et structure Data.frame**
"""

import xlrd
import pandas as pd
df = pd.read_excel("/content/credit_risk_dataset.xlsx")
#affichage des données
print(df)

"""**Description des données**"""

#dimension
print(df.shape)
#nombre d'individus
n = df.shape[0]
#nombre de variables
p = df.shape[1]
print(n)
print(p)

#afficher les premières lignes du jeu de données
print(df.head())

#description des données
print(df.describe(include='all'))

"""**Nettoyage des données **

"""

#Identifier les valeurs manquantes :
print(df.isnull().sum())
print(df.columns)

import pandas as pd
import numpy as np
quantitative_columns=['person_age', 'person_income','person_emp_length','loan_amnt','loan_int_rate','loan_status', 'loan_percent_income','cb_person_cred_hist_length']
# 1. Identifier les colonnes problématiques
print("Types de données avant traitement:")
print(df[quantitative_columns].dtypes)

# 2. Nettoyer spécifiquement la colonne loan_int_rate
if 'loan_int_rate' in df.columns:
    # Supprimer les caractères non numériques (%, espaces, etc.)
    df['loan_int_rate'] = df['loan_int_rate'].astype(str).str.replace('%', '').str.strip()

    # Convertir en numérique, les erreurs deviendront NaN
    df['loan_int_rate'] = pd.to_numeric(df['loan_int_rate'], errors='coerce')

# 3. Liste finale des colonnes purement numériques
numeric_cols = df[quantitative_columns].select_dtypes(include=['int64', 'float64']).columns.tolist()
print(f"\nColonnes numériques retenues pour le traitement: {numeric_cols}")

# 4. Remplissage des NA par la moyenne groupée
group_col = 'person_home_ownership'
for col in numeric_cols:
    df[col] = df.groupby(group_col)[col].transform(lambda x: x.fillna(x.mean()))

# 5. Vérification finale
print("\nAPRÈS REMPLACEMENT")
print("Nombre de valeurs manquantes par colonne :")
print(df[numeric_cols].isnull().sum())
print("\nAperçu des données:")
print(df[numeric_cols].head())

"""**Standardisation des variables quantitatives**

"""

# Histogrammes pour les variables quantitatives
import matplotlib.pyplot as plt # Import matplotlib.pyplot
quantitative_vars = ['person_age', 'person_income','person_emp_length','loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income','loan_status']
df[quantitative_vars].hist(bins=20, figsize=(12, 6))
plt.suptitle('Distribution des Variables Quantitatives')
plt.tight_layout()
plt.show()

# Boîtes à moustaches pour détecter les outliers
import seaborn as sns # Make sure seaborn is imported # Import the seaborn library and assign it to the alias 'sns'

plt.figure(figsize=(12, 6))
sns.boxplot(data=df[quantitative_vars])
plt.title('Boîtes à Moustaches pour les Variables Quantitatives')
plt.xticks(rotation=45)
plt.show()

# Import the StandardScaler class from sklearn.preprocessing
from sklearn.preprocessing import StandardScaler

# Standardisation des données
scaler = StandardScaler()
# Use the correct variable name: quantitative_columns
X_std = scaler.fit_transform(df[quantitative_columns])
print(df)
print ("\n   Les variables standardisées    ")
print(X_std)

"""**Analyse visuelle de la matrice de corrélation entre les variables standardisées**"""

# 2. Analyse de la matrice de corrélation
import matplotlib.pyplot as plt  # Import matplotlib.pyplot at the beginning

corr_matrix = pd.DataFrame(X_std, columns=quantitative_columns).corr()
plt.figure(figsize=(10, 8))
import seaborn as sns # Make sure seaborn is imported
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title("Matrice de corrélations")
plt.show()

# 3. Réalisation de l'ACP
from sklearn.decomposition import PCA # Import the PCA class

pca = PCA()
pca.fit(X_std)

"""**Calcul de valeur propres **"""

# Valeurs propres
eigenvalues = pca.explained_variance_
print("Valeurs propres :")
for i, val in enumerate(eigenvalues, 1):
    print(f"Composante {i}: {val:.4f}")
plt.figure(figsize=(10, 5))
plt.bar(range(1, len(eigenvalues)+1), eigenvalues, alpha=0.5, align='center')
plt.plot(range(1, len(eigenvalues)+1), eigenvalues, '-o')
plt.title('Eboulis des valeurs propres')
plt.xlabel('Composantes principales')
plt.ylabel('Valeurs propres')
plt.axhline(y=1, color='r', linestyle='--') # Kaiser criterion
plt.show()

#variance expliquée
inert=pca.explained_variance_ratio_
# Variance expliquée (en %)
inertcum = pca.explained_variance_ratio_ * 100
print("\nVariance expliquée (%):")
for i, var in enumerate(inertcum, 1):
   { print(f"Composante {i}: {var:.2f}%")}
print(pd.DataFrame({'valprop':eigenvalues,'inertie':inert,'inertiecum':inertcum, }))

# Vecteurs propres
components = pca.components_
features = quantitative_columns
print("\nVecteurs propres :")
components_df = pd.DataFrame(components,
                           columns=features,
                           index=[f'F{i+1}' for i in range(len(features))])
print(components_df)

# Contribution des variables aux axes
plt.figure(figsize=(12, 6))
sns.heatmap(components_df.T.abs(), annot=True, cmap='YlOrRd')
plt.title("Contributions absolues des variables aux composantes")
plt.show()

# les projections des données sur les vecteurs propres
coord = pca.fit_transform(X_std)

# quantitative_columns should have already been defined

# Create a list to store the component values for F1 and F2
F1_values = coord[:, 0]
F2_values = coord[:, 1]
Cos2_F1_values = F1_values**2
Cos2_F2_values = F2_values**2

results_individus = pd.DataFrame({
    'F1': F1_values,
    'F2': F2_values,
    'Cos2_F1': Cos2_F1_values,
    'Cos2_F2': Cos2_F2_values
})
results_individus['Total_Cos2'] = results_individus['Cos2_F1'] + results_individus['Cos2_F2']

print("\n Tableau des projections des individus (extrait) :")
print(results_individus.head())

# 4. Choix de la dimension (SEV de projection)
import numpy as np # Make sure numpy is imported
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(1, len(eigenvalues)+1), eigenvalues, alpha=0.6)
plt.axhline(y=1, color='r', linestyle='--')
plt.title('Critère de Kaiser (λ > 1)')
plt.xlabel('Composantes')
plt.ylabel('Valeurs propres')

plt.subplot(1, 2, 2)
# Assign the explained variance ratio to the variable 'explained_variance'
explained_variance = pca.explained_variance_ratio_
plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance), 'o-')
plt.axhline(y=0.8, color='g', linestyle='--')
plt.title('Variance expliquée cumulée')
plt.xlabel('Nombre de composantes')
plt.ylabel('Variance cumulée')
plt.tight_layout()
plt.show()

# 5. Projection des individus avec identifiants
plt.figure(figsize=(14, 10))
principal_components = pca.transform(X_std) # Calculate principal components

plt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.6)

# Ajout des étiquettes pour chaque point

    #print(f"Ligne {i} : ID = {txt}")

for i, txt in enumerate(df.index):  # Utilisez 'Brand' si vous voulez les marques
    plt.annotate(txt,
                (principal_components[i, 0], principal_components[i, 1]),
                textcoords="offset points",
                xytext=(5,5),
                ha='center',
                fontsize=8)

plt.xlabel(f'F1 ({inert[0]*100:.1f}%)')
plt.ylabel(f'F2 ({inert[1]*100:.1f}%)')
plt.title('Projection des individus avec identifiants', pad=20)
plt.grid()
plt.tight_layout()
plt.show()

# Cercle des corrélations
def plot_correlation_circle(features, components, explained_variance):
    fig, ax = plt.subplots(figsize=(10, 10))
    for i, feature in enumerate(features):
        ax.arrow(0, 0,
                components[0, i],
                components[1, i],
                head_width=0.05, head_length=0.05,
                fc='k', ec='k')
        ax.text(components[0, i]*1.15,
               components[1, i]*1.15,
               feature, color='k', ha='center', va='center')

    circle = plt.Circle((0,0), 1, color='blue', fill=False)
    ax.add_artist(circle)
    ax.set_xlim(-1.1, 1.1)
    ax.set_ylim(-1.1, 1.1)
    ax.set_xlabel(f'F1 ({explained_variance[0]*100:.1f}%)')
    ax.set_ylabel(f'F2 ({explained_variance[1]*100:.1f}%)')
    ax.set_title('Cercle des corrélations', pad=20)
    ax.grid()
    plt.show()

plot_correlation_circle(quantitative_columns, pca.components_[:2], explained_variance)

# Définir le nombre de composantes à utiliser
n_components = 2

# Fonction pour calculer les contributions
def compute_contributions(pca, n_components):
    contrib = pd.DataFrame(
        pca.components_[:n_components].T**2 * pca.explained_variance_[:n_components],
        # Utilise 'quantitative_columns' au lieu de 'quantitative_vars'
        index=quantitative_columns,
        columns=[f'F{i+1}' for i in range(n_components)]
    )
    return contrib / contrib.sum(axis=0) * 100

# Calcul des contributions et des qualités de représentation
contrib_var = compute_contributions(pca, n_components)
cos2_var = pd.DataFrame(
    pca.components_[:n_components].T**2,
    # Utilise 'quantitative_columns' au lieu de 'quantitative_vars'
    index=quantitative_columns,
    columns=[f'F{i+1}' for i in range(n_components)]
)

# Affichage des résultats
print("\n=== CONTRIBUTIONS DES VARIABLES ===")
print(contrib_var)
print("\n=== QUALITÉS DE REPRÉSENTATION (cos²) ===")
print(cos2_var)

# 7. Interprétation complète
print("\n=== SYNTHÈSE D'INTERPRÉTATION ===")
print(f"Les 2 premières composantes expliquent {np.sum(explained_variance[:2])*100:.1f}% de la variance totale.")
print("\nStructure des composantes principales :")
structure = pd.DataFrame(
    pca.components_[:2].T,
    # Use 'quantitative_columns' instead of 'quantitative_vars'
    index=quantitative_columns,
    columns=['F1', 'F2']
)
print(structure)

print("\nVariables les plus contributives :")
print("F1 :", contrib_var['F1'].idxmax(), f"({contrib_var['F1'].max():.1f}%)")
print("F2 :", contrib_var['F2'].idxmax(), f"({contrib_var['F2'].max():.1f}%)")

import pandas as pd
import numpy as np

# 1. Variables qualitatives à traiter
cat_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']

# 2. Affichage des valeurs manquantes avant traitement
print("Valeurs manquantes avant imputation :")
print(df[cat_cols].isnull().sum())

# 3. Imputation intelligente par mode conditionnel
def impute_qualitative(df):
    # Copie du dataframe pour éviter les warnings
    df_imputed = df.copy()

    # a. Remplacer les chaînes vides par NaN
    df_imputed[cat_cols] = df_imputed[cat_cols].replace('', np.nan)

    # b. Imputation par le mode conditionnel (par person_home_ownership)
    for col in cat_cols:
        # Créer un dictionnaire {person_home_ownership: mode}
        mode_by_ownership = df_imputed.groupby('person_home_ownership')[col].agg(
            lambda x: x.mode()[0] if not x.mode().empty else np.nan
        ).to_dict()

        # Appliquer l'imputation
        df_imputed[col] = df_imputed.apply(
            lambda row: mode_by_ownership.get(row['person_home_ownership'], np.nan)
            if pd.isna(row[col])
            else row[col],
            axis=1
        )

    # c. Fallback : mode global si mode par person_home_ownership indisponible
    global_modes = df_imputed[cat_cols].mode().iloc[0]
    df_imputed[cat_cols] = df_imputed[cat_cols].fillna(global_modes)

    return df_imputed

# 4. Application
df_imputed = impute_qualitative(df)

# 5. Vérification
print("\nValeurs manquantes après imputation :")
print(df_imputed[cat_cols].isnull().sum())

# 6. Statistiques des remplacements
print("\nDétail des imputations :")
for col in cat_cols:
    original_nulls = df[col].isnull().sum()
    imputed_nulls = df_imputed[col].isnull().sum()
    replaced = original_nulls - imputed_nulls
    print(f"→ {col} : {replaced} valeurs remplacées")

import pandas as pd

# Charger et afficher sans formatage
pd.set_option('display.max_columns', None)  # Affiche toutes les colonnes
print(df_imputed.head(20))  # Affiche les 20 premières lignes # Use df_imputed instead of df_clean

!pip install prince

!pip install --upgrade prince # Upgrade prince to the latest version

import pandas as pd
import prince
import matplotlib.pyplot as plt
from IPython.display import display

# 3. Sélection et nettoyage des variables qualitatives
qualitative_vars = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
df_qual = df[qualitative_vars].fillna("Missing").astype(str)

# 4. Encodage One-Hot avec get_dummies
dummies = pd.get_dummies(df_qual, prefix_sep="::")

# 5. Calcul de la matrice de Burt
burt_matrix = dummies.T @ dummies

# 6. Affichage clair de la matrice de Burt
display(
    burt_matrix.style
        .background_gradient(cmap="YlOrRd", axis=None)
        .format(lambda x: "{:.0f}".format(x)) ) # Correction ici

#ACP dual
import pandas
#Stat - librairie numpy
import numpy as np
#scikit-learn
import sklearn
#scree plot
import matplotlib.pyplot as plt

#chargement de la première feuille de données,
#première ligne avec noms des variables
#et première colonne identifiant(index) des individus
Tab = pandas.read_excel("/content/credit_risk_dataset.xlsx")

X = pandas.get_dummies(Tab)

#dimension
print(X.shape)
#nombre d'observations
n = X.shape[0]
#nombre de variables
p = X.shape[1]
print(n)
print(p)
pandas.set_option('display.max_columns', None)

#classe StandardScaler pour standardisation (centrage et reduction)
from sklearn.preprocessing import StandardScaler
#instanciation
sc = StandardScaler()
#transformation – centrage-réduction
Z = sc.fit_transform(X)
print(Z)

#classe pour l'ACP
from sklearn.decomposition import PCA
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
#instanciation
acp = PCA()
print(acp)
#calculs des composantes principales (nouvelles coordonnées)
Z = sc.fit_transform(X)
imputer = SimpleImputer(strategy='mean') # Create an imputer instance
X_imputed = imputer.fit_transform(X) # Impute missing values in X


X_imputed = pandas.DataFrame(X_imputed, columns=X.columns, index=X.index)

sc = StandardScaler()
Z = sc.fit_transform(X_imputed)

acp = PCA()
coord = acp.fit_transform(Z)

# Transformer un tableau en DataFrame
coorddf=pandas.DataFrame(coord,index=X.index)
print(coorddf)

#variances expliquées qui sont les valeurs propres
eigval=acp.explained_variance_
#proportion de variance expliquée
inert=acp.explained_variance_ratio_
cuminert=(np.cumsum(acp.explained_variance_ratio_))*100
#valeurs propres triées
eigval=np.sort(acp.explained_variance_)[::-1]
inert=acp.explained_variance_ratio_*100
print(pandas.DataFrame({'valprop':eigval,'inertie':inert,'inertiecum':cuminert, }))

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

# Supposons que Z est votre matrice centrée-réduite
acp = PCA()
coord = acp.fit_transform(Z)

# Nombre réel de composantes disponibles
n_components = coord.shape[1]
print(f"Nombre de composantes principales disponibles : {n_components}")

# Variances expliquées
eigval = acp.explained_variance_
n = coord.shape[0]

# Calcul des métriques
A = coord**2
normind = np.sum(Z**2, axis=1)

# Créer des tableaux de la bonne taille
max_components = min(10, n_components)  # Prendre le minimum entre 10 et le nombre réel de composantes
Ctr = np.zeros((n, max_components))
Cos = np.zeros((n, max_components))

# Calcul pour chaque axe disponible
for i in range(max_components):
    Ctr[:, i] = (A[:, i] / (n * eigval[i])) * 100
    Cos[:, i] = (A[:, i] / normind) * 100

# Préparation des colonnes pour le DataFrame
columns_dict = {'ID': X.index}
for i in range(max_components):
    columns_dict[f'Coord_{i+1}'] = coord[:, i]
    columns_dict[f'Contr_{i+1}'] = Ctr[:, i]
    columns_dict[f'Coscar_{i+1}'] = Cos[:, i]

# Création du DataFrame
resultat = pd.DataFrame(columns_dict)

# Affichage
pd.set_option('display.max_rows', None)
print(resultat)

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

# Supposons que Z est votre matrice centrée-réduite
acp = PCA()
coord = acp.fit_transform(Z)

# Nombre réel de composantes disponibles
n_components = coord.shape[1]
print(f"Nombre de composantes principales disponibles : {n_components}")

# Variances expliquées
eigval = acp.explained_variance_
n = coord.shape[0]

# Calcul des métriques
A = coord**2
normind = np.sum(Z**2, axis=1)

# Créer des tableaux de la bonne taille
max_components = min(10, n_components)  # Prendre le minimum entre 10 et le nombre réel de composantes
Ctr = np.zeros((n, max_components))
Cos = np.zeros((n, max_components))

# Calcul pour chaque axe disponible
for i in range(max_components):
    Ctr[:, i] = (A[:, i] / (n * eigval[i])) * 100
    Cos[:, i] = (A[:, i] / normind) * 100

# Préparation des colonnes pour le DataFrame
columns_dict = {'ID': X.index}
for i in range(max_components):
    columns_dict[f'Coord_{i+1}'] = coord[:, i]
    columns_dict[f'Contr_{i+1}'] = Ctr[:, i]
    columns_dict[f'Coscar_{i+1}'] = Cos[:, i]

# Création du DataFrame
resultat = pd.DataFrame(columns_dict)

# Affichage
pd.set_option('display.max_rows', None)
print(resultat)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
import pandas as pd
from matplotlib.patches import Circle, Patch

# Supposons que vous avez déjà:
# X = votre DataFrame numérique (sans variables catégorielles)
# acp = PCA().fit(X)  # ACP déjà calculée

# 1. Préparation des données
corvar = np.transpose(acp.components_)  # Corrélations variables-facteurs
n_components = acp.n_components_  # Nombre réel de composantes

# 2. Configuration de la visualisation
facteur_agrandissement = 1.5
n_pairs = min(5, n_components//2)  # Nombre de paires à afficher (max 5)
fig, axes = plt.subplots((n_pairs + 1)//2, 2, figsize=(16, 5*((n_pairs + 1)//2)))
axes = axes.ravel()  # Pour un accès linéaire

# 3. Palette de couleurs
colors = plt.cm.tab20(np.linspace(0, 1, X.shape[1]))

# 4. Création des graphiques
for i in range(n_pairs):
    ax = axes[i]
    axis1, axis2 = 2*i, 2*i + 1

    # Vérification qu'on ne dépasse pas le nombre de composantes
    if axis2 >= n_components:
        axis2 = axis1  # Si nombre impair de composantes

    # Coordonnées des variables
    x = corvar[:, axis1] * facteur_agrandissement
    y = corvar[:, axis2] * facteur_agrandissement

    # Points colorés
    for j in range(X.shape[1]):
        ax.scatter(x[j], y[j], color=colors[j], s=100, alpha=0.7)

    # Ajout des noms des variables
    for j in range(X.shape[1]):
        ax.text(x[j], y[j], X.columns[j],
                fontsize=10, ha='center', va='bottom')

    # Axes et cercle
    ax.axhline(0, color='silver', linestyle='--', linewidth=1)
    ax.axvline(0, color='silver', linestyle='--', linewidth=1)
    ax.add_artist(Circle((0, 0), facteur_agrandissement,
                 color='blue', fill=False, linewidth=1))

    # Configuration des axes
    ax.set_xlim(-facteur_agrandissement*1.1, facteur_agrandissement*1.1)
    ax.set_ylim(-facteur_agrandissement*1.1, facteur_agrandissement*1.1)
    ax.set_xlabel(f'Axe {axis1+1} ({acp.explained_variance_ratio_[axis1]:.1%})')
    ax.set_ylabel(f'Axe {axis2+1} ({acp.explained_variance_ratio_[axis2]:.1%})')
    ax.set_title(f'Plan factoriel {axis1+1}-{axis2+1}')

# 5. Gestion des sous-graphiques inutilisés
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

# 6. Légende globale
legend_elements = [Patch(facecolor=colors[j], label=X.columns[j])
                  for j in range(X.shape[1])]
fig.legend(handles=legend_elements, loc='lower center',
           ncol=min(5, X.shape[1]), fontsize=10)

plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Ajustement pour la légende
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import matplotlib.pyplot as plt
import numpy as np

# Sélection des colonnes numériques
num_cols = ['person_age', 'person_income','person_emp_length','loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income','loan_status']
X = df[num_cols]

# Standardisation
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Remplacement des valeurs infinies ou NaN
for i in range(X_std.shape[1]):
    col = X_std[:, i]
    invalid_mask = np.logical_or(np.isinf(col), np.isnan(col))
    if np.any(invalid_mask):
        col_mean = np.mean(col[~invalid_mask])
        X_std[invalid_mask, i] = col_mean

# CAH avec lien de Ward
Z = linkage(X_std, method='ward')

# Affichage du dendrogramme
plt.figure(figsize=(10, 6))
dendrogram(Z)
plt.title('Dendrogramme - CAH')
plt.xlabel('Index des observations')
plt.ylabel('Distance')
plt.grid(True)
plt.tight_layout()
plt.show()

# Découpage en clusters
df['Cluster_CAH'] = fcluster(Z, t=8, criterion='distance')

# Générer des noms génériques : C1, C2, ..., Cn
unique_clusters = np.unique(df['Cluster_CAH'])
cluster_names = {k: f"C{k}" for k in unique_clusters}

# Ajout des noms de clusters au DataFrame
df['Cluster_Name'] = df['Cluster_CAH'].map(cluster_names)

# Calcul des centroïdes
centroids = df.groupby('Cluster_CAH')[num_cols].mean()
centroids_std = scaler.transform(centroids)  # Centroides standardisés

# Affichage avec annotations
plt.figure(figsize=(12, 7))
scatter = plt.scatter(X_std[:, 0], X_std[:, 1], c=df['Cluster_CAH'], cmap='tab10', alpha=0.6, label='Points')
plt.scatter(centroids_std[:, 0], centroids_std[:, 1], c='red', s=100, marker='X', label='Centroïdes')

# Ajout des noms génériques de clusters
for i, (x, y) in enumerate(centroids_std[:, :2]):
    cluster_id = unique_clusters[i]
    cluster_label = cluster_names[cluster_id]
    plt.text(x, y, cluster_label,
             fontsize=9, weight='bold',
             color='red', ha='center', va='center',
             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))

plt.title('Segmentation des incomes par Cluster')
plt.xlabel('person_income (standardisé)')
plt.ylabel('person_age(standardisé)')

plt.legend()
plt.colorbar(scatter, label='Cluster ID')
plt.grid(True)
plt.tight_layout()
plt.show()

# Caractérisation des clusters avec les noms
cluster_stats = df.groupby('Cluster_Name')[num_cols].agg(['mean', 'std'])
print("Caractérisation des clusters CAH :")
display(cluster_stats)

from sklearn.cluster import KMeans # Import the KMeans class

inerties = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_std)
    inerties.append(kmeans.inertia_)

plt.plot(range(1, 10), inerties, marker='o')
plt.xlabel('Nombre de clusters')
plt.ylabel('Inertie intra-classe')
plt.title('Méthode du coude')
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_score # Import silhouette_score

# Apply k-means clustering
from sklearn.cluster import KMeans # Import the KMeans class
kmeans = KMeans(n_clusters=4, random_state=42)
df['Cluster_kmeans'] = kmeans.fit_predict(X_std)

# 4. Visualisation K-means seul
plt.figure(figsize=(10, 6))
plt.scatter(X_std[:, 0], X_std[:, 1],
            c=df['Cluster_kmeans'],
            cmap='viridis',
            alpha=0.6,
            label='Points')

plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            c='red',
            marker='X',
            s=200,
            label='Centroïdes K-means')

plt.title('Résultats du K-means (4 clusters)')
plt.xlabel('person_income (standardisé)')
plt.ylabel('person_age (standardisé)')
plt.colorbar(label='Cluster K-means')
plt.legend()
plt.grid(True)
plt.show()

# 5. Comparaison côte à côte K-means vs CAH
# Préparation des centroïdes CAH
centroids = df.groupby('Cluster_CAH')[num_cols].mean()
centroids_std = scaler.transform(centroids)

# Création de la figure comparative
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Graphique K-means
sc1 = ax1.scatter(X_std[:, 0], X_std[:, 1],
                 c=df['Cluster_kmeans'],
                 cmap='viridis',
                 alpha=0.6)
ax1.scatter(kmeans.cluster_centers_[:, 0],
           kmeans.cluster_centers_[:, 1],
           c='red',
           marker='X',
           s=200)
ax1.set_title('K-means Clustering')
ax1.set_xlabel('person_age (standardisé)')
ax1.set_ylabel('person_income (standardisé)')
plt.colorbar(sc1, ax=ax1, label='Cluster K-means')

# Graphique CAH
sc2 = ax2.scatter(X_std[:, 0], X_std[:, 1],
                 c=df['Cluster_CAH'],
                 cmap='viridis',
                 alpha=0.6)
ax2.scatter(centroids_std[:, 0],
           centroids_std[:, 1],
           c='red',
           marker='X',
           s=200)
ax2.set_title('CAH Clustering')
ax2.set_xlabel('person_age (standardisé)')
plt.colorbar(sc2, ax=ax2, label='Cluster CAH')

plt.tight_layout()
plt.show()

# 6. Analyse des différences (optionnel)
df['Comparison'] = 'Identique'
df.loc[df['Cluster_kmeans'] != df['Cluster_CAH'], 'Comparison'] = 'Différent'

plt.figure(figsize=(10, 6))
plt.scatter(X_std[:, 0], X_std[:, 1],
            c=df['Comparison'].map({'Identique':'green', 'Différent':'red'}),
            alpha=0.6)
plt.title('Points classés différemment par K-means et CAH')
plt.xlabel('Mileage (standardisé)')
plt.ylabel('Price (standardisé)')
plt.legend(handles=[
    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Identique'),
    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Différent')
])
plt.grid(True)
plt.show()

# Calcul du pourcentage d'accord
agreement = (df['Comparison'] == 'Identique').mean()
print(f"Pourcentage d'accord entre les méthodes: {agreement:.1%}")

# 7. Évaluation des clusters (optionnel)
print("\nScores de silhouette:")
print(f"K-means: {silhouette_score(X_std, df['Cluster_kmeans']):.3f}")
print(f"CAH: {silhouette_score(X_std, df['Cluster_CAH']):.3f}")

# 8. Statistiques par cluster (optionnel)
print("\nStatistiques K-means:")
print(df.groupby('Cluster_kmeans')[num_cols].agg(['mean', 'std']))

print("\nStatistiques CAH:")
print(df.groupby('Cluster_CAH')[num_cols].agg(['mean', 'std']))

!pip install pandas numpy scikit-learn matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    auc
)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# Chargement du dataset
data = pd.read_excel("/content/credit_risk_dataset.xlsx")

# Correction spéciale pour la colonne loan_int_rate
def convert_loan_int_rate(value):
    try:
        # Essayer de convertir en float (pour les taux d'intérêt)
        return float(value)
    except:
        # Si échec, essayer de convertir en datetime puis en timestamp
        try:
            return pd.to_datetime(value).timestamp()
        except:
            # Si les deux échouent, retourner NaN
            return np.nan

# Appliquer la conversion à la colonne loan_int_rate
data['loan_int_rate'] = data['loan_int_rate'].apply(convert_loan_int_rate)

# Suppression des valeurs manquantes
data = data.dropna()

# Encodage des variables catégorielles
cat_cols = ["person_home_ownership", "loan_intent", "loan_grade", "cb_person_default_on_file"]
label_encoders = {}

for col in cat_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col].astype(str))  # Conversion en string pour éviter les erreurs
    label_encoders[col] = le

# Séparation des features et de la cible
X = data.drop("loan_status", axis=1)
y = data["loan_status"]

# Vérification des types de données
print("Types de données avant normalisation:")
print(X.dtypes)

# Normalisation des données numériques
scaler = StandardScaler()

# Sélection uniquement des colonnes numériques pour la normalisation
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# Séparation en train/test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nPrétraitement terminé avec succès!")

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Prédictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # Probabilités pour ROC-AUC

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-Default", "Default"], yticklabels=["Non-Default", "Default"])
plt.xlabel("Prédit")
plt.ylabel("Réel")
plt.title("Matrice de confusion")
plt.show()

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Précision : {precision:.2f}")
print(f"Rappel : {recall:.2f}")
print(f"F1-Score : {f1:.2f}")

# Rapport complet
print("\nRapport de classification :")
print(classification_report(y_test, y_pred))

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de faux positifs (FPR)')
plt.ylabel('Taux de vrais positifs (TPR)')
plt.title('Courbe ROC')
plt.legend(loc="lower right")
plt.show()

precision_pr, recall_pr, _ = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall_pr, precision_pr)

plt.figure(figsize=(8, 6))
plt.plot(recall_pr, precision_pr, color='green', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')
plt.xlabel('Rappel (Recall)')
plt.ylabel('Précision')
plt.title('Courbe Précision-Rappel')
plt.legend(loc="upper right")
plt.show()

# Trouver le meilleur seuil pour maximiser F1-Score
best_threshold = thresholds[np.argmax(tpr - fpr)]
print(f"Meilleur seuil : {best_threshold:.2f}")

# Prédictions avec nouveau seuil
y_pred_optimized = (y_prob > best_threshold).astype(int)

# Nouvelle évaluation
print("\nNouvelle évaluation avec seuil optimisé :")
print(classification_report(y_test, y_pred_optimized))
